{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Systems\n",
    "In a multi-agent system individual agents \n",
    " - with different personas, \n",
    " - tool sets can \n",
    " - communicate, \n",
    " - collaborate, \n",
    " - add additional context, \n",
    " - and do work in parallel. \n",
    "\n",
    "They often have special rules for how to work as a team, decide things together, and handle disagreements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install packages\n",
    "%pip install pyautogen\n",
    "%pip install openai\n",
    "%pip install chroma\n",
    "%pip install python-dotenv\n",
    "%pip install markdownify\n",
    "%pip install apify_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import autogen\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# Save your OPENAI_API_KEY in a .env file in the same directory as this script\n",
    "llm_config_dict = {\"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-agent chat: \n",
    " - The simplest form of conversation pattern where two agents chat with each other using Autogen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/autogen_image1.png\" alt=\"GroupChat Flow\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "student_agent = ConversableAgent(\n",
    "    name=\"Student_Agent\",\n",
    "    system_message=\"You are a student willing to learn. You ask meaningful and precise follow up questions and are eager to learn more.\",\n",
    "    llm_config=llm_config_dict,\n",
    ")\n",
    "teacher_agent = ConversableAgent(\n",
    "    name=\"Teacher_Agent\",\n",
    "    system_message=\"You are an expert at Large Language model research and you teach cutting edge AI development technologies.\",\n",
    "    llm_config=llm_config_dict,\n",
    ")\n",
    "\n",
    "chat_result = student_agent.initiate_chat(\n",
    "    teacher_agent,\n",
    "    message=\"How to prove or test that Artificial Intelligence is actually intelligent?\",\n",
    "    summary_method=\"reflection_with_llm\",\n",
    "    max_turns=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the summary. It is contained within the chat_result object of type ChatResult, which was returned by the initiate_chat method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat_result.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, the summary method is configured to `reflection_with_llm`, which processes a list of conversation messages and summarizes them by invoking an LLM.\n",
    "\n",
    "Initially, the summary method attempts to utilize the recipient’s LLM; if it is unavailable, it defaults to the sender’s LLM.\n",
    "\n",
    "Here, the recipient is “Teacher_Agent” and the sender is “Student_Agent”.\n",
    "\n",
    "The input prompt for the LLM is the following default prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ConversableAgent.DEFAULT_SUMMARY_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential chat: \n",
    " - a series of conversations between two agents, linked by a carryover mechanism that transfers the summary of the prior chat to the context of the subsequent chat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/autogen_image2.png\" alt=\"GroupChat Flow\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import ConversableAgent\n",
    "\n",
    "report_agent = ConversableAgent(\n",
    "    name=\"Report_Agent\",\n",
    "    system_message=\"You are responsible for creating a report by extracting insights from the chat history.\",\n",
    "    llm_config=llm_config_dict,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# The Researcher Agent discovers the potential of AGI.\n",
    "researcher_agent = ConversableAgent(\n",
    "    name=\"Researcher_Agent\",\n",
    "    system_message=\"You explore and describe the potential capabilities and advancements of AGI.\",\n",
    "    llm_config=llm_config_dict,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# The Ethicist Agent evaluates the ethical implications of AGI.\n",
    "ethicist_agent = ConversableAgent(\n",
    "    name=\"Ethicist_Agent\",\n",
    "    system_message=\"You evaluate the ethical implications of AGI based on the research findings.\",\n",
    "    llm_config=llm_config_dict,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# The Economist Agent analyzes the economic impact of AGI.\n",
    "economist_agent = ConversableAgent(\n",
    "    name=\"Economist_Agent\",\n",
    "    system_message=\"You analyze the economic impact of AGI based on the ethical evaluations.\",\n",
    "    llm_config=llm_config_dict,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# The Policy Maker Agent develops policies based on the findings of the previous agents.\n",
    "policy_maker_agent = ConversableAgent(\n",
    "    name=\"Policy_Maker_Agent\",\n",
    "    system_message=\"You develop policies to manage AGI based on the economic analysis.\",\n",
    "    llm_config=llm_config_dict,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a sequence of two-agent chats.\n",
    "# Each element in the list is a dictionary that specifies the arguments\n",
    "# for the initiate_chat method.\n",
    "\n",
    "chat_results = report_agent.initiate_chats(\n",
    "    [\n",
    "        {\n",
    "            \"recipient\": researcher_agent,\n",
    "            \"message\": \"Discover the potential of AGI\",\n",
    "            \"max_turns\": 1,\n",
    "            \"summary_method\": \"last_msg\",\n",
    "        },\n",
    "        {\n",
    "            \"recipient\": ethicist_agent,\n",
    "            \"message\": \"Evaluate the ethical implications of AGI\",\n",
    "            \"max_turns\": 1,\n",
    "            \"summary_method\": \"last_msg\",\n",
    "        },\n",
    "        {\n",
    "            \"recipient\": economist_agent,\n",
    "            \"message\": \"Analyze the economic impact of AGI\",\n",
    "            \"max_turns\": 1,\n",
    "            \"summary_method\": \"last_msg\",\n",
    "        },\n",
    "        {\n",
    "            \"recipient\": policy_maker_agent,\n",
    "            \"message\": \"Develop policies to manage AGI\",\n",
    "            \"max_turns\": 1,\n",
    "            \"summary_method\": \"last_msg\",\n",
    "        }\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First Chat Summary: \", chat_results[0].summary)\n",
    "print(\"Second Chat Summary: \", chat_results[1].summary)\n",
    "print(\"Third Chat Summary: \", chat_results[2].summary)\n",
    "print(\"Fourth Chat Summary: \", chat_results[3].summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool Use\n",
    "In this section, we'll demonstrate tool-use capability of the agentic system. Web Search and RAG are popular capabilities used by people every day, we show how to use AutoGen to do:\n",
    "\n",
    "* Web search using Bing API. \n",
    "* Web scraping using Apify API\n",
    "* Question and answering with RAG.\n",
    "\n",
    "More info can be found here : https://microsoft.github.io/autogen/docs/tutorial/tool-use "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Search using Bing API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your Bing API key in the .env file in the same directory as this script\n",
    "BING_API_KEY = os.environ[\"BING_API_KEY\"]\n",
    "\n",
    "llm_config = {\n",
    "    \"timeout\": 600,\n",
    "    \"cache_seed\": 44,  # change the seed for different trials\n",
    "    \"config_list\": llm_config_dict['config_list'],\n",
    "    \"temperature\": 0,\n",
    "}\n",
    "\n",
    "summarizer_llm_config = {\n",
    "    \"timeout\": 600,\n",
    "    \"cache_seed\": 44,  # change the seed for different trials\n",
    "    \"config_list\": llm_config_dict['config_list'],\n",
    "    \"temperature\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen.agentchat.contrib.web_surfer import WebSurferAgent\n",
    "\n",
    "web_surfer = WebSurferAgent(\n",
    "    \"web_surfer\",\n",
    "    llm_config=llm_config,\n",
    "    summarizer_llm_config=summarizer_llm_config,\n",
    "    browser_config={\"viewport_size\": 4096, \"bing_api_key\": BING_API_KEY},\n",
    ")\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    \"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False,\n",
    "    default_auto_reply=\"\",\n",
    "    is_termination_msg=lambda x: True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1 = \"\"\"\n",
    "What is the history of Boise, Idaho?\n",
    "\"\"\"\n",
    "\n",
    "user_proxy.initiate_chat(web_surfer, message=task1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task2 = \"Click on the first link.\"\n",
    "user_proxy.initiate_chat(web_surfer, message=task2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task3 = \"What is the population of Boise, Idaho?\"\n",
    "user_proxy.initiate_chat(web_surfer, message=task3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping Using Apify Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your Apify API key in the .env file in the same directory as this script\n",
    "APIFY_API_KEY = os.environ[\"APIFY_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apify_client import ApifyClient\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "\n",
    "def scrape_page(url: Annotated[str, \"The URL of the web page to scrape\"]) -> Annotated[str, \"Scraped content\"]:\n",
    "    # Initialize the ApifyClient with your API token\n",
    "    client = ApifyClient(token=APIFY_API_KEY)\n",
    "\n",
    "    # Prepare the Actor input\n",
    "    run_input = {\n",
    "        \"startUrls\": [{\"url\": url}],\n",
    "        \"useSitemaps\": False,\n",
    "        \"crawlerType\": \"playwright:firefox\",\n",
    "        \"includeUrlGlobs\": [],\n",
    "        \"excludeUrlGlobs\": [],\n",
    "        \"ignoreCanonicalUrl\": False,\n",
    "        \"maxCrawlDepth\": 0,\n",
    "        \"maxCrawlPages\": 1,\n",
    "        \"initialConcurrency\": 0,\n",
    "        \"maxConcurrency\": 200,\n",
    "        \"initialCookies\": [],\n",
    "        \"proxyConfiguration\": {\"useApifyProxy\": True},\n",
    "        \"maxSessionRotations\": 10,\n",
    "        \"maxRequestRetries\": 5,\n",
    "        \"requestTimeoutSecs\": 60,\n",
    "        \"dynamicContentWaitSecs\": 10,\n",
    "        \"maxScrollHeightPixels\": 5000,\n",
    "        \"removeElementsCssSelector\": \"\"\"nav, footer, script, style, noscript, svg,\n",
    "    [role=\\\"alert\\\"],\n",
    "    [role=\\\"banner\\\"],\n",
    "    [role=\\\"dialog\\\"],\n",
    "    [role=\\\"alertdialog\\\"],\n",
    "    [role=\\\"region\\\"][aria-label*=\\\"skip\\\" i],\n",
    "    [aria-modal=\\\"true\\\"]\"\"\",\n",
    "        \"removeCookieWarnings\": True,\n",
    "        \"clickElementsCssSelector\": '[aria-expanded=\"false\"]',\n",
    "        \"htmlTransformer\": \"readableText\",\n",
    "        \"readableTextCharThreshold\": 100,\n",
    "        \"aggressivePrune\": False,\n",
    "        \"debugMode\": True,\n",
    "        \"debugLog\": True,\n",
    "        \"saveHtml\": True,\n",
    "        \"saveMarkdown\": True,\n",
    "        \"saveFiles\": False,\n",
    "        \"saveScreenshots\": False,\n",
    "        \"maxResults\": 9999999,\n",
    "        \"clientSideMinChangePercentage\": 15,\n",
    "        \"renderingTypeDetectionPercentage\": 10,\n",
    "    }\n",
    "\n",
    "    # Run the Actor and wait for it to finish\n",
    "    run = client.actor(\"aYG0l9s7dbB7j3gbS\").call(run_input=run_input)\n",
    "\n",
    "    # Fetch and print Actor results from the run's dataset (if there are any)\n",
    "    text_data = \"\"\n",
    "    for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "        text_data += item.get(\"text\", \"\") + \"\\n\"\n",
    "\n",
    "    average_token = 0.75\n",
    "    max_tokens = 20000  # slightly less than max to be safe 32k\n",
    "    text_data = text_data[: int(average_token * max_tokens)]\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import ConversableAgent, register_function\n",
    "\n",
    "# Create web scrapper agent.\n",
    "scraper_agent = ConversableAgent(\n",
    "    \"WebScraper\",\n",
    "    llm_config=llm_config_dict,\n",
    "    system_message=\"You are a web scrapper and you can scrape any web page using the tools provided. \"\n",
    "    \"Returns 'TERMINATE' when the scraping is done.\",\n",
    ")\n",
    "\n",
    "# Create user proxy agent.\n",
    "user_proxy_agent = ConversableAgent(\n",
    "    \"UserProxy\",\n",
    "    llm_config=False,  # No LLM for this agent.\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False,  # No code execution for this agent.\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") is not None and \"terminate\" in x[\"content\"].lower(),\n",
    "    default_auto_reply=\"Please continue if not finished, otherwise return 'TERMINATE'.\",\n",
    ")\n",
    "\n",
    "# Register the function with the agents.\n",
    "register_function(\n",
    "    scrape_page,\n",
    "    caller=scraper_agent,\n",
    "    executor=user_proxy_agent,\n",
    "    name=\"scrape_page\",\n",
    "    description=\"Scrape a web page and return the content.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Boise,_Idaho'\n",
    "chat_result = user_proxy_agent.initiate_chat(\n",
    "    scraper_agent,\n",
    "    message=f\"Can you scrape {url} for me?\",\n",
    "    summary_method=\"reflection_with_llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boise_idaho_wiki = chat_result.chat_history[-2]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boise_idaho_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"boise_idaho_wiki_content.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(boise_idaho_wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform RAG on the Scraped Wiki File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\n",
    "from autogen import AssistantAgent\n",
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
    "import chromadb\n",
    "\n",
    "DOC_PATH = [\"./boise_idaho_wiki_content.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = {'config_list': [{'model': 'gpt-3.5-turbo', 'api_key': os.environ[\"OPENAI_API_KEY\"]}], 'timeout': 60, 'cache_seed': 88, 'temperature': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    system_message=\"You are a helpful assistant.\",\n",
    "    llm_config=llm_config\n",
    ")\n",
    "        \n",
    "ragproxyagent = RetrieveUserProxyAgent(\n",
    "    name=\"ragproxyagent\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    retrieve_config={\n",
    "        \"task\": \"qa\",\n",
    "        \"docs_path\": DOC_PATH,\n",
    "        \"chunk_token_size\": 1000,\n",
    "        \"model\": llm_config_dict[\"config_list\"][0][\"model\"],\n",
    "        \"client\": chromadb.PersistentClient(path=\"/tmp/chromadb\"),\n",
    "        \"collection_name\": \"boise_idaho\",\n",
    "        \"get_or_create\": True, # if True, will create/return a collection for the retrieve chat.\n",
    "    },\n",
    "    code_execution_config={\"use_docker\": False}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the population of Boise, Idaho?\"\n",
    "res = ragproxyagent.initiate_chat(assistant, message=ragproxyagent.message_generator, problem=prompt, n_results=1, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Question:{prompt}\")\n",
    "print(f\"Answer from RAG: {res.chat_history[-1]['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Chat \n",
    "\n",
    "Up to this point, we have encountered conversation patterns that include either two agents or a series of two-agent interactions. \n",
    "\n",
    "AutoGen introduces a broader conversation model known as **group chat**, which includes more than two agents. \n",
    "\n",
    "The fundamental concept of group chat is that all agents participate in a unified conversation thread and share the same context. \n",
    "\n",
    "This approach is beneficial for tasks that necessitate collaboration among several agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./images/autogen_image3.png\" alt=\"GroupChat Flow\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Group Chat Manager orchestrates a group chat with the following steps:\n",
    "\n",
    "1. The Group Chat Manager selects an agent to speak.\n",
    "2. The selected agent speaks and the message is sent back to the Group Chat Manager.\n",
    "3. The Group Chat Manager broadcasts the message to all other agents in the group.\n",
    "4. This process repeats until the conversation stops.\n",
    "\n",
    "Group Chat Manager can use several strategies to select the next agent :\n",
    "\n",
    "- **round_robin**: The Group Chat Manager selects agents in a round-robin fashion based on the order of the agents provided.\n",
    "- **random**: The Group Chat Manager selects agents randomly.\n",
    "- **manual**: The Group Chat Manager selects agents by asking for human input.\n",
    "- **auto**: The default strategy, which selects agents using the Group Chat Manager’s LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from autogen import ConversableAgent, GroupChat, GroupChatManager, UserProxyAgent\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"User_proxy\",\n",
    "    system_message=\"A human admin.\",\n",
    "    code_execution_config={\n",
    "        \"last_n_messages\": 2,\n",
    "        \"work_dir\": \"groupchat\",\n",
    "        \"use_docker\": False,\n",
    "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
    "    human_input_mode=\"TERMINATE\",\n",
    ")\n",
    "\n",
    "economist = ConversableAgent(\n",
    "    name=\"Economist\",\n",
    "    system_message=\"You are a highly experienced economist specializing in global economic trends and geopolitical risks.\",\n",
    "    llm_config=llm_config_dict,\n",
    "    description=\"Expert in global economics and the impact of conflicts.\"\n",
    ")\n",
    "\n",
    "capitalist = ConversableAgent(\n",
    "    name=\"Capitalist\",\n",
    "    system_message=\"You are a pragmatic capitalist focused on market opportunities and risks, even in extreme circumstances.\",\n",
    "    llm_config=llm_config_dict,\n",
    "    description=\"Astute observer of market dynamics amidst global crises.\"\n",
    ")\n",
    "\n",
    "environmentalist = ConversableAgent(\n",
    "    name=\"Environmentalist\",\n",
    "    system_message=\"You are an environmentalist advocating for sustainable practices and conservation of natural resources.\",\n",
    "    llm_config=llm_config_dict,\n",
    "    description=\"Promotes environmental awareness and sustainable solutions.\"\n",
    ")\n",
    "\n",
    "# Group Chat Setup\n",
    "group_chat = GroupChat(\n",
    "    agents=[user_proxy, economist, capitalist, environmentalist],\n",
    "    messages=[],\n",
    "    max_round=5,\n",
    "    send_introductions=True,\n",
    ")\n",
    "\n",
    "# Group Chat Manager\n",
    "group_chat_manager = GroupChatManager(\n",
    "    groupchat=group_chat,\n",
    "    llm_config=llm_config_dict\n",
    ")\n",
    "\n",
    "# Initiate the Conversation\n",
    "chat_result = user_proxy.initiate_chat(\n",
    "    group_chat_manager,\n",
    "    message=\"Analyze the potential global ramifications of onset of Artificial general intelligence. Summerise your insights.\",\n",
    "    # summary_method=\"reflection_with_llm\"\n",
    ")\n",
    "\n",
    "# Print the Discussion Summary Report\n",
    "print(chat_result.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to https://microsoft.github.io/autogen/docs/ for information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
